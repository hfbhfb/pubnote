---
title: k8s日志,监控与应用管理
date: 2022-05-30 00:22:55
permalink: /pages/ed73639/
categories:
  - k8s
  - CKA认证学习k8s认证
tags:
  - 
---


## bilibili Kubernetes CKA 认证培训 (华为云)

## CloudNativeLive 回录 **原理部分** **CKA加群**

## 大纲
监控集群组件
监控应用
管理组件日志
管理应用日志
Deployment升级和回滚
配置应用的不同方法
应用弹性伸缩
应用自恢复


## 监控集群组件
kubectl cluster-info
kubectl cluster-info dump  #内容太多

## 通过插件部署
kubectl get pod etcd -n kube-system #这个运行不了

kubectl describe pod kube-apiserver -n kube-system
kubectl describe pod etcd -n kube-system


## 组件Metrics
curl -k https://localhost:10250/stats/summary  ## 没有被认证 Unauthorized
sudo cat /etc/kubernetes/kubelet.conf
sudo curl -k --key /var/lib/kubelet/pki/kubelet-client-current.pem --cert /var/lib/kubelet/pki/kubelet-client-current.pem  https://localhost:10250/stats/summary ## Forbidden (user=system:node:master, verb=get, resource=nodes, subresource=stats)


## 组件健康状态:
sudo curl -k --key /var/lib/kubelet/pki/kubelet-client-current.pem --cert /var/lib/kubelet/pki/kubelet-client-current.pem  -k https://10.1.14.122:443/healthz 
sudo curl -k --key /var/lib/kubelet/pki/kubelet-client-current.pem --cert /var/lib/kubelet/pki/kubelet-client-current.pem  -k https://10.1.14.122:443/stats/summary ## Forbidden (user=system:node:master, verb=get, resource=nodes, subresource=proxy)

## Heapster + cAdvisor 监控集群组件
cAdvisor 既能收集容器CPU,内存,文件系统和网络使用统计信息,还能采集节点资源使用情况
cAdvisor和Heapster都不能进行数据存储,趋势分析和报警.因此,还需要将数据推送到InfluxDB,Grafana等后端进行存储和图形化展示.Heapster即将被metrics-server替代


## 查看过去一段时间做一次聚合平均值
kubectl top node


## k8s Dashboard UI (不考)


## 展示Pod Cpu/内存/存储资源消耗
kubectl top pod {pod name}


## 管理K8s组件日志
- 组件日志
/var/log/container
/var/log/pods
- 使用systemd管理
journalctl -u kubelet.service
- 使用k8s插件部署
kubectl logs -h
kubectl logs deploy/nginx3 ## Found 6 pods, using pod/nginx3-59475d8756-r7sk8

## 管理k8s应用日志
- 从容器标准输出截获
kubectl logs -f {pod name} -c {container name}
docker logs -f {docker name}
- 直接进入容器内查看日志
kubectl exec -it {pod} -c {container} /bin/sh
docker exec -it -c {container} /bin/sh

## 日志文件挂载到主机目录 **必考**
kubectl explain pod.spec.containers
kubectl explain pod.spec.containers.volumeDevices
kubectl explain pod.spec.volumes


## Deployment升级与回滚 -1
kubectl create {deployment} {deploy-name} -image={image} - replicas={rep.}
kubectl create deployment nginx --image nginx --port 80 --replicas=2


## 升级策略
minReadySeconds:5
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1 #默认为25%
    maxUnavailable:1 #默认为25%
```yaml
spec:
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
```
- 暂停/恢复Deployment
kubectl rollout pause deployment/nginx3
kubectl rollout resume deployment/nginx3
- 查询升级状态
kubectl rollout status deployment/nginx3
- 查询升级历史状态
kubectl rollout history deployment/nginx3
kubectl rollout history deployment/nginx3 -oyaml
kubectl rollout history deployment/nginx3 --revision=3
- 回滚和指定版本回滚
kubectl rollout undo deployment/nginx3 
kubectl rollout undo deployment/nginx3 --to-revision=3

## kubectl rollout history deployment/nginx3 --revision=3
```yaml
deployment.apps/nginx3 with revision #3
Pod Template:
  Labels:       app=nginx3
        pod-template-hash=df594ff5c
  Containers:
   nginx:
    Image:      nginx:1.22
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
```

## 应用弹性伸缩
kubectl scale deployment/nginx3 --replicas=3

## 对接Heapster(metric-server),和HPA联动后
kubectl autoscale deployment/nginx3 --min=4 --max=6 --cpu-percent=60


## 应用自恢复: restartPolicy + livenessProbe **存活探针**
- restartPolicy: Always, OnFailure,Never
- livenessProbe: http/https Get,shell exec, tcpSocket
- 实例测试
kubectl create deployment mydeploy2 --image nginx --port 80 --replicas=1 -o yaml --dry-run=client > mydeploy2.yaml
kubectl explain pod.spec
kubectl explain pod.spec.containers
kubectl explain pod.spec.containers.livenessProbe
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: mydeploy2
  name: mydeploy2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mydeploy2
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: mydeploy2
    spec:
      restartPolicy: Always
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources: {}
        livenessProbe:
          initialDelaySeconds: 15
          tcpSocket: 
            port: 8760 
          periodSeconds: 20
status: {}
```


## 遗留的问题 **原生块设备**
kubectl explain pod.spec.containers.volumeDevices
** 区别 **volumeDevices** **volumeMounts**(etcd使用的)

